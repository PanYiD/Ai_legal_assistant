# app.py
# -*- coding: utf-8 -*-
import os
import json
import time
from pathlib import Path
from typing import List, Dict, Optional

import streamlit as st
import chromadb

from llama_index.core import VectorStoreIndex, StorageContext, Settings, PromptTemplate
from llama_index.core.schema import TextNode
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

# ================== é¡µé¢ä¸æ¨¡æ¿ ==================
st.set_page_config(page_title="AIæ³•å¾‹åŠ©æ‰‹", page_icon="ğŸ¦œğŸ”—", layout="wide")
st.title("ğŸ¦œğŸ”—  AIæ³•å¾‹åŠ©æ‰‹")

QA_TEMPLATE = (
    "<|im_start|>system\n"
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ¡æ–‡å›ç­”é—®é¢˜ï¼š\n"
    "ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼š\n{context_str}\n<|im_end|>\n"
    "<|im_start|>user\n{query_str}<|im_end|>\n"
    "<|im_start|>assistant\n"
)
response_template = PromptTemplate(QA_TEMPLATE)

# ================== é…ç½®åŒº ==================
class Config:
    EMBED_MODEL_PATH = os.getenv("EMBED_MODEL_PATH", r"/mnt/workspace/LLM/BAAI/bge-small-zh-v1___5")
    LLM_MODEL_PATH   = os.getenv("LLM_MODEL_PATH",   r"/mnt/workspace/LLM/Qwen/Qwen3-4B-Instruct-2507")

    DATA_DIR      = os.getenv("DATA_DIR", "./data")
    VECTOR_DB_DIR = os.getenv("VECTOR_DB_DIR", "./chroma_db")
    PERSIST_DIR   = os.getenv("PERSIST_DIR", "./storage")

    COLLECTION_NAME = os.getenv("COLLECTION_NAME", "chinese_labor_laws")
    TOP_K = int(os.getenv("TOP_K", "3"))

# ================== æ•°æ®å¤„ç† ==================
def load_and_validate_json_files(data_dir: str) -> List[Dict]:
    """åŠ è½½å¹¶éªŒè¯JSONæ³•å¾‹æ–‡ä»¶"""
    json_files = list(Path(data_dir).glob("*.json"))
    if not json_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°JSONæ–‡ä»¶äº {data_dir}")

    all_data = []
    for json_file in json_files:
        with open(json_file, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                # éªŒè¯æ•°æ®ç»“æ„
                if not isinstance(data, list):
                    raise ValueError(f"æ–‡ä»¶ {json_file.name} æ ¹å…ƒç´ åº”ä¸ºåˆ—è¡¨")
                for item in data:
                    if not isinstance(item, dict):
                        raise ValueError(f"æ–‡ä»¶ {json_file.name} åŒ…å«éå­—å…¸å…ƒç´ ")
                    for k, v in item.items():
                        if not isinstance(v, str):
                            raise ValueError(f"æ–‡ä»¶ {json_file.name} ä¸­é”® '{k}' çš„å€¼ä¸æ˜¯å­—ç¬¦ä¸²")
                all_data.extend({
                    "content": item,
                    "metadata": {"source": json_file.name}
                } for item in data)
            except Exception as e:
                raise RuntimeError(f"åŠ è½½æ–‡ä»¶ {json_file} å¤±è´¥: {str(e)}")
    return all_data

def create_nodes(raw_data: List[Dict]) -> List[TextNode]:
    """æ·»åŠ IDç¨³å®šæ€§ä¿éšœ"""
    nodes: List[TextNode] = []
    for entry in raw_data:
        law_dict = entry["content"]
        source_file = entry["metadata"]["source"]
        for full_title, content in law_dict.items():
            node_id = f"{source_file}::{full_title}"  # ç¨³å®šID
            parts = full_title.split(" ", 1)
            law_name = parts[0] if len(parts) > 0 else "æœªçŸ¥æ³•å¾‹"
            article  = parts[1] if len(parts) > 1 else "æœªçŸ¥æ¡æ¬¾"
            node = TextNode(
                text=content,
                id_=node_id,
                metadata={
                    "law_name": law_name,
                    "article": article,
                    "full_title": full_title,
                    "source_file": source_file,
                    "content_type": "legal_article"
                }
            )
            nodes.append(node)
    return nodes

# ================== å‘é‡å­˜å‚¨ / ç´¢å¼• ==================
def init_vector_store(nodes: Optional[List[TextNode]], force_rebuild: bool = False) -> VectorStoreIndex:
    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)

    # å¯é€‰ï¼šå¼ºåˆ¶é‡å»ºæ—¶æ¸…ç©ºæ—§é›†åˆ
    if force_rebuild:
        try:
            chroma_client.delete_collection(Config.COLLECTION_NAME)
            st.toast(f"å·²åˆ é™¤æ—§é›†åˆï¼š{Config.COLLECTION_NAME}", icon="ğŸ—‘ï¸")
        except Exception:
            pass

    chroma_collection = chroma_client.get_or_create_collection(
        name=Config.COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )

    storage_context = StorageContext.from_defaults(
        vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
    )

    need_build = (chroma_collection.count() == 0) and (nodes is not None)

    if need_build:
        st.info(f"åˆ›å»ºæ–°ç´¢å¼•ï¼ˆ{len(nodes)} ä¸ªèŠ‚ç‚¹ï¼‰...")
        # ï¼ˆå¯é€‰ï¼‰ä»¥ä¸‹ä¸€è¡Œä¸æ˜¯å¿…é¡»ï¼›VectorStoreIndex ä¼šå†™ docstoreï¼Œè¿™é‡Œåªæ˜¯æ¼”ç¤ºæ˜¾å¼æ·»åŠ 
        storage_context.docstore.add_documents(nodes)

        index = VectorStoreIndex(
            nodes,
            storage_context=storage_context,
            show_progress=True
        )
        # åŒé‡æŒä¹…åŒ–ä¿éšœ
        storage_context.persist(persist_dir=Config.PERSIST_DIR)
        index.storage_context.persist(persist_dir=Config.PERSIST_DIR)
    else:
        st.info("åŠ è½½å·²æœ‰ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(
            persist_dir=Config.PERSIST_DIR,
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
        )
        index = VectorStoreIndex.from_vector_store(
            storage_context.vector_store,
            storage_context=storage_context,
            embed_model=Settings.embed_model
        )

    # ç»Ÿè®¡ä¿¡æ¯
    try:
        doc_count = len(storage_context.docstore.docs)
        st.caption(f"DocStoreè®°å½•æ•°ï¼š{doc_count} | Chromaå‘é‡æ¡æ•°ï¼š{chroma_collection.count()}")
    except Exception:
        pass

    return index

# ================== æ¨¡å‹åˆå§‹åŒ–ï¼ˆç¼“å­˜ï¼‰ ==================
@st.cache_resource(show_spinner=True)
def init_models(embed_path: str, llm_path: str):
    """åˆå§‹åŒ–æ¨¡å‹å¹¶åœ¨ Settings ä¸­æ³¨å†Œã€‚è¿”å› True è¡¨ç¤ºæˆåŠŸã€‚"""
    embed_model = HuggingFaceEmbedding(
        model_name=embed_path,
        # å¦‚éœ€å½’ä¸€åŒ–æˆ–è®¾å¤‡æŒ‡å®šï¼Œå¯åœ¨ä¸åŒç‰ˆæœ¬ä¼ å‚
        # encode_kwargs={"normalize_embeddings": True}
    )
    Settings.embed_model = embed_model

    llm = HuggingFaceLLM(
        model_name=llm_path,
        tokenizer_name=llm_path,
        model_kwargs={
            "trust_remote_code": True,
            # "device_map": "auto"
        },
        tokenizer_kwargs={"trust_remote_code": True},
        generate_kwargs={"temperature": 0.3}
    )
    Settings.llm = llm

    # éªŒè¯
    try:
        test_embedding = embed_model.get_text_embedding("æµ‹è¯•æ–‡æœ¬")
        st.caption(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦ï¼š{len(test_embedding)}")
    except Exception as e:
        st.error(f"åµŒå…¥æ¨¡å‹éªŒè¯å¤±è´¥ï¼š{e}")

    return True

@st.cache_resource(show_spinner=True)
def build_query_engine(
    data_dir: str,
    force_rebuild: bool,
    top_k: int
):
    """æ„å»ºæˆ–è½½å…¥ç´¢å¼•ï¼Œå¹¶è¿”å› query_engineã€‚"""
    # è‹¥å‘é‡åº“ç›®å½•ä¸å­˜åœ¨æˆ–é€‰æ‹©å¼ºåˆ¶é‡å»ºï¼Œåˆ™åŠ è½½æ•°æ®
    nodes: Optional[List[TextNode]] = None
    need_data = force_rebuild or (not Path(Config.VECTOR_DB_DIR).exists())

    if need_data:
        raw = load_and_validate_json_files(data_dir)
        nodes = create_nodes(raw)

    index = init_vector_store(nodes, force_rebuild=force_rebuild)

    query_engine = index.as_query_engine(
        similarity_top_k=top_k,
        text_qa_template=response_template,
        verbose=False
    )
    return query_engine

# ================== ä¾§è¾¹æ å‚æ•° ==================
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    embed_path = st.text_input("Embedding æ¨¡å‹è·¯å¾„", Config.EMBED_MODEL_PATH)
    llm_path   = st.text_input("LLM æ¨¡å‹è·¯å¾„",       Config.LLM_MODEL_PATH)
    data_dir   = st.text_input("æ•°æ®ç›®å½•ï¼ˆ*.jsonï¼‰",  Config.DATA_DIR)

    top_k = st.number_input("Top-Kï¼ˆå¬å›æ¡æ•°ï¼‰", 1, 20, Config.TOP_K, 1)
    force_rebuild = st.checkbox("å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆæ¸…ç©ºé›†åˆåé‡å»ºï¼‰", value=False)

    init_btn = st.button("åˆå§‹åŒ– / é‡æ–°åŠ è½½")

# ================== åˆå§‹åŒ–ï¼ˆæ¨¡å‹ & ç´¢å¼•ï¼‰ ==================
if "query_engine" not in st.session_state or init_btn:
    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ä¸ç´¢å¼•..."):
        try:
            init_models(embed_path, llm_path)
            st.session_state["query_engine"] = build_query_engine(
                data_dir=data_dir,
                force_rebuild=force_rebuild,
                top_k=int(top_k),
            )
            st.success("ç´¢å¼•å°±ç»ª âœ…")
        except Exception as e:
            st.error(f"åˆå§‹åŒ–å¤±è´¥ï¼š{e}")

# ================== å¯¹è¯åŒº ==================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ AI å°æ³•ã€‚è¯·æå‡ºä½ çš„æ³•å¾‹é—®é¢˜ï¼ˆæˆ‘å°†ä¸¥æ ¼ä¾æ®æ³•å¾‹æ¡æ–‡ä½œç­”ï¼‰ã€‚"}
    ]

# å±•ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "èŠå¤©è®°å½•å·²æ¸…ç©ºã€‚è¯·æå‡ºä½ çš„æ³•å¾‹é—®é¢˜ã€‚"}
    ]
st.sidebar.button("ğŸ§¹ æ¸…ç©ºèŠå¤©è®°å½•", on_click=clear_chat_history)

# ç”¨æˆ·è¾“å…¥
prompt = st.chat_input("è¯·è¾“å…¥æ³•å¾‹ç›¸å…³é—®é¢˜â€¦")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# ç”Ÿæˆå›ç­”
if st.session_state.messages and st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        if "query_engine" not in st.session_state or st.session_state["query_engine"] is None:
            st.error("æ£€ç´¢å¼•æ“å°šæœªå°±ç»ªï¼Œè¯·å…ˆåœ¨ä¾§è¾¹æ å®Œæˆåˆå§‹åŒ–ã€‚")
        else:
            with st.spinner("æ€è€ƒä¸­..."):
                t0 = time.time()
                try:
                    resp = st.session_state["query_engine"].query(prompt)
                    answer_text = getattr(resp, "response", str(resp))
                    st.markdown(answer_text)

                    # æº¯æºå±•ç¤º
                    if getattr(resp, "source_nodes", None):
                        st.markdown("**æ”¯æŒä¾æ®ï¼š**")
                        for i, node in enumerate(resp.source_nodes, 1):
                            meta = node.metadata or {}
                            sc = getattr(node, "score", None)
                            title = meta.get("full_title", "æœªçŸ¥æ ‡é¢˜")
                            with st.expander(f"[{i}] {title}  | ç›¸ä¼¼åº¦ï¼š{sc}"):
                                st.write(f"æ¥æºæ–‡ä»¶ï¼š{meta.get('source_file', '-')}")
                                st.write(f"æ³•å¾‹åç§°ï¼š{meta.get('law_name', '-')}")
                                st.write(f"æ¡æ¬¾ï¼š{meta.get('article', '-')}")
                                st.write("---")
                                st.write(node.text)

                    st.caption(f"è€—æ—¶ï¼š{time.time()-t0:.2f}s")
                except Exception as e:
                    st.error(f"æ£€ç´¢æˆ–ç”Ÿæˆå¤±è´¥ï¼š{e}")
                    st.session_state.messages.append({"role": "assistant", "content": f"æ£€ç´¢æˆ–ç”Ÿæˆå¤±è´¥ï¼š{e}"})
                    st.stop()

    # æŠŠæœ€ç»ˆå›ç­”åŠ å…¥å¯¹è¯å†å²
    st.session_state.messages.append({"role": "assistant", "content": answer_text})


# ================== ä½¿ç”¨è¯´æ˜ ==================
with st.sidebar.expander("ğŸ“¦ ä½¿ç”¨è¯´æ˜", expanded=False):
    st.markdown(        """
**æ•°æ®æ ¼å¼**ï¼š`./data/*.json`ï¼Œæ¯ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…ƒç´ æ˜¯å­—å…¸ï¼Œç¤ºä¾‹ï¼š
```json
[
  {"åŠ³åŠ¨æ³• ç¬¬ä¸‰åå…­æ¡": "â€¦â€¦æ¡æ–‡å†…å®¹â€¦â€¦"},
  {"åŠ³åŠ¨æ³• ç¬¬å››åä¸€æ¡": "â€¦â€¦æ¡æ–‡å†…å®¹â€¦â€¦"}
]
 """
)


