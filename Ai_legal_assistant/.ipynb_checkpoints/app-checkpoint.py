# app.py
# -*- coding: utf-8 -*-
import os
import json
import time
from pathlib import Path
from typing import List, Dict, Optional

import streamlit as st

import chromadb
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    Settings,
    PromptTemplate,
)
from llama_index.core.schema import TextNode
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.node_parser import TokenTextSplitter
from llama_index.core.postprocessor import SimilarityPostprocessor

# ------------------------ é¡µé¢é…ç½® ------------------------
st.set_page_config(page_title="RAG_demo", page_icon="ğŸ¦œğŸ”—", layout="wide")
st.title("ğŸ¦œğŸ”—  AIæ³•å¾‹åŠ©æ‰‹")

# ------------------------ é…ç½®åŒº ------------------------
class Config:
    # æ¨¡å‹ï¼ˆæŒ‰ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
    EMBED_MODEL_PATH = os.getenv("EMBED_MODEL_PATH", "/mnt/workspace/LLM/BAAI/bge-small-zh-v1___5")
    LLM_MODEL_PATH = os.getenv("LLM_MODEL_PATH", "/mnt/workspace/LLM/Qwen/Qwen3-4B-Instruct-2507")

    # æ•°æ® & æŒä¹…åŒ–
    DATA_DIR = os.getenv("DATA_DIR", "./data")                # æ”¾ *.json æ³•æ¡çš„ç›®å½•
    VECTOR_DB_DIR = os.getenv("VECTOR_DB_DIR", "./chroma_db") # Chroma å‘é‡åº“
    PERSIST_DIR = os.getenv("PERSIST_DIR", "./storage")       # LlamaIndex å­˜å‚¨

    COLLECTION_NAME = os.getenv("COLLECTION_NAME", "chinese_labor_laws")
    DEFAULT_TOP_K = int(os.getenv("TOP_K", "5"))
    DEFAULT_CUTOFF = float(os.getenv("SIM_CUTOFF", "0.75"))

# ------------------------ æ¨¡æ¿ ------------------------
LEGAL_QA_TMPL = """
ä½ æ˜¯ä¸¥æ ¼çš„æ³•å¾‹åŠ©ç†ã€‚ä»…ä¾æ®â€œå·²æ£€ç´¢åˆ°çš„æ³•å¾‹æ¡æ–‡â€å›ç­”ä¸­æ–‡é—®é¢˜ï¼š
- ä¸è¦è¾“å‡ºä¸æ¡æ–‡æ— å…³çš„å†…å®¹ï¼Œä¸è¦è‡†æµ‹ã€‚
- å¦‚æ¡æ–‡ä¸è¶³ä»¥å›ç­”ï¼Œè¯·ç­”ï¼šâ€œæœªåœ¨å·²æä¾›çš„æ³•å¾‹æ¡æ–‡ä¸­æ‰¾åˆ°æ˜ç¡®ä¾æ®ã€‚â€
- å›å¤æ—¶å°½é‡å¼•ç”¨ç›¸å…³æ³•åç§°ä¸æ¡å·ï¼ˆè‡ªç„¶è¯­è¨€è¡¨è¿°ï¼Œæ— éœ€é“¾æ¥ï¼‰ã€‚

ã€å·²æ£€ç´¢åˆ°çš„æ³•å¾‹æ¡æ–‡ã€‘ï¼š
{context_str}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{query_str}

ã€ä½ çš„å›ç­”ã€‘ï¼š
""".strip()
RESPONSE_TEMPLATE = PromptTemplate(LEGAL_QA_TMPL)

# ------------------------ æ•°æ®åŠ è½½&æ ¡éªŒ ------------------------
def load_and_validate_json_files(data_dir: str) -> List[Dict]:
    json_files = list(Path(data_dir).glob("*.json"))
    if not json_files:
        raise FileNotFoundError(f"æœªåœ¨ {data_dir} æ‰¾åˆ°ä»»æ„ JSON æ–‡ä»¶")

    all_data = []
    for jf in json_files:
        with open(jf, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if not isinstance(data, list):
                    raise ValueError(f"æ–‡ä»¶ {jf.name} æ ¹å…ƒç´ åº”ä¸ºåˆ—è¡¨")
                for item in data:
                    if not isinstance(item, dict):
                        raise ValueError(f"æ–‡ä»¶ {jf.name} åŒ…å«éå­—å…¸å…ƒç´ ")
                    for k, v in item.items():
                        if not isinstance(v, str) or not v.strip():
                            raise ValueError(f"{jf.name} ä¸­é”® '{k}' çš„å€¼ä¸æ˜¯éç©ºå­—ç¬¦ä¸²")
                all_data.extend({"content": item, "metadata": {"source": jf.name}} for item in data)
            except Exception as e:
                raise RuntimeError(f"åŠ è½½æ–‡ä»¶ {jf} å¤±è´¥: {str(e)}")
    return all_data

def build_nodes(raw_data: List[Dict]) -> List[TextNode]:
    nodes: List[TextNode] = []
    for entry in raw_data:
        law_dict = entry["content"]
        src = entry["metadata"]["source"]
        for full_title, content in law_dict.items():
            node_id = f"{src}::{full_title}"
            parts = full_title.split(" ", 1)
            law_name = parts[0] if len(parts) > 0 else "æœªçŸ¥æ³•å¾‹"
            article = parts[1] if len(parts) > 1 else "æœªçŸ¥æ¡æ¬¾"
            nodes.append(
                TextNode(
                    text=content.strip(),
                    id_=node_id,
                    metadata={
                        "law_name": law_name,
                        "article": article,
                        "full_title": full_title,
                        "source_file": src,
                        "content_type": "legal_article",
                    },
                )
            )
    return nodes

def chunk_nodes(nodes: List[TextNode], chunk_size: int = 512, chunk_overlap: int = 64) -> List[TextNode]:
    splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunked: List[TextNode] = []
    for n in nodes:
        pieces = splitter.split_text(n.text)
        for i, t in enumerate(pieces):
            chunked.append(
                TextNode(
                    text=t,
                    id_=f"{n.id_}#chunk{i}",
                    metadata=dict(n.metadata),
                )
            )
    return chunked

# ------------------------ å‘é‡åº“ä¸ç´¢å¼• ------------------------
def create_or_load_index(
    nodes: Optional[List[TextNode]],
    force_rebuild: bool,
    top_k: int,
    sim_cutoff: float,
):
    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)

    # å¼ºåˆ¶é‡å»ºï¼šåˆ é™¤æ—§é›†åˆé¿å…é‡å¤
    if force_rebuild:
        try:
            chroma_client.delete_collection(Config.COLLECTION_NAME)
            st.toast(f"å·²åˆ é™¤æ—§é›†åˆï¼š{Config.COLLECTION_NAME}", icon="ğŸ—‘ï¸")
        except Exception:
            pass

    chroma_collection = chroma_client.get_or_create_collection(
        name=Config.COLLECTION_NAME, metadata={"hnsw:space": "cosine"}
    )

    storage_context = StorageContext.from_defaults(
        vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
    )

    need_build = (chroma_collection.count() == 0) and (nodes is not None)

    if force_rebuild or need_build:
        st.info(f"å¼€å§‹åˆ›å»ºæ–°ç´¢å¼•ï¼ˆ{0 if nodes is None else len(nodes)} ä¸ªèŠ‚ç‚¹ï¼‰...")
        index = VectorStoreIndex(nodes or [], storage_context=storage_context, show_progress=True)
        storage_context.persist(persist_dir=Config.PERSIST_DIR)
    else:
        st.info("åŠ è½½å·²æœ‰ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(
            persist_dir=Config.PERSIST_DIR,
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection),
        )
        index = VectorStoreIndex.from_vector_store(
            storage_context.vector_store,
            storage_context=storage_context,
            embed_model=Settings.embed_model,
        )

    st.caption(f"DocStoreè®°å½•æ•°ï¼š{len(storage_context.docstore.docs)} | Chromaå‘é‡æ¡æ•°ï¼š{chroma_collection.count()}")
    # ç»„è£… query_engineï¼ˆåŠ å…¥æ¨¡æ¿ä¸ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
    query_engine = index.as_query_engine(
        similarity_top_k=max(3, top_k),
        response_mode="compact",
        text_qa_template=RESPONSE_TEMPLATE,
        node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=sim_cutoff)],
        verbose=False,
    )
    return query_engine

# ------------------------ æ¨¡å‹åˆå§‹åŒ–ï¼ˆç¼“å­˜ï¼‰ ------------------------
@st.cache_resource(show_spinner=True)
def init_models(
    embed_path: str,
    llm_path: str,
):
    # åµŒå…¥æ¨¡å‹ï¼ˆbge ç³»åˆ—å»ºè®® cosine + å½’ä¸€åŒ–ï¼›ä¸åŒç‰ˆæœ¬å¯èƒ½å‚æ•°åä¸åŒï¼Œè¿™é‡Œä¿æŒé»˜è®¤ï¼‰
    embed_model = HuggingFaceEmbedding(
        model_name=embed_path,
        device="cuda",
    )
    Settings.embed_model = embed_model

    # ç”Ÿæˆæ¨¡å‹
    llm = HuggingFaceLLM(
        model_name=llm_path,
        tokenizer_name=llm_path,
        model_kwargs={
            "trust_remote_code": True,
            "device_map": "auto",
        },
        tokenizer_kwargs={"trust_remote_code": True},
        generate_kwargs={"temperature": 0.3, "max_new_tokens": 768},
    )
    Settings.llm = llm

    # ç®€è¦éªŒè¯
    try:
        test_vec = embed_model.get_text_embedding("æµ‹è¯•æ–‡æœ¬")
        st.caption(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦ï¼š{len(test_vec)}")
    except Exception as e:
        st.error(f"åµŒå…¥æ¨¡å‹éªŒè¯å¤±è´¥ï¼š{e}")

    return True  # ä»…å ä½

# ------------------------ ä¾§è¾¹æ é…ç½® ------------------------
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    embed_path = st.text_input("Embedding æ¨¡å‹è·¯å¾„", Config.EMBED_MODEL_PATH)
    llm_path = st.text_input("LLM æ¨¡å‹è·¯å¾„", Config.LLM_MODEL_PATH)
    data_dir = st.text_input("æ•°æ®ç›®å½•ï¼ˆ*.jsonï¼‰", Config.DATA_DIR)
    chunk_size = st.number_input("åˆ†å—é•¿åº¦ï¼ˆtokensï¼‰", 128, 2048, 512, 16)
    chunk_overlap = st.number_input("åˆ†å—é‡å ", 0, 512, 64, 8)
    top_k = st.number_input("Top-K", 1, 20, Config.DEFAULT_TOP_K, 1)
    sim_cutoff = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆè¶Šé«˜è¶Šä¸¥æ ¼ï¼‰", 0.0, 0.99, Config.DEFAULT_CUTOFF, 0.01)
    force_rebuild = st.checkbox("å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆæ¸…ç©ºé›†åˆåé‡å»ºï¼‰", value=False)
    init_btn = st.button("åˆå§‹åŒ– / é‡æ–°åŠ è½½")

# ------------------------ åˆå§‹åŒ–æµç¨‹ ------------------------
if "query_engine" not in st.session_state or init_btn:
    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹ä¸ç´¢å¼•..."):
        # 1) æ¨¡å‹
        init_models(embed_path, llm_path)

        # 2) æ•°æ®ï¼ˆä»…åœ¨éœ€è¦é‡å»ºæˆ–åº“ä¸ºç©ºæ—¶åŠ è½½ï¼‰
        raw_data = build_nodes(chunk_nodes(build_nodes(load_and_validate_json_files(data_dir)), chunk_size, chunk_overlap))  # å ä½é¿å… mypy è­¦å‘Š
        # ä¸Šé¢å†™åœ¨ä¸€è¡Œå¯è¯»æ€§å·®ï¼Œå®é™…åˆ†æ­¥æ‰§è¡Œæ›´æ¸…æ™°ï¼š
        try:
            raw = load_and_validate_json_files(data_dir)
            nodes = build_nodes(raw)
            nodes = chunk_nodes(nodes, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
            nodes = None

        # 3) ç´¢å¼•
        try:
            st.session_state["query_engine"] = create_or_load_index(
                nodes=nodes, force_rebuild=force_rebuild, top_k=int(top_k), sim_cutoff=float(sim_cutoff)
            )
            st.success("ç´¢å¼•å°±ç»ª âœ…")
        except Exception as e:
            st.error(f"ç´¢å¼•åˆ›å»º/åŠ è½½å¤±è´¥ï¼š{e}")

# ------------------------ å¯¹è¯æ¡† & åŠŸèƒ½ ------------------------
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ AI å°èšã€‚è¯·æå‡ºä½ çš„æ³•å¾‹é—®é¢˜ï¼ˆä»…ä¾æ®å·²å¯¼å…¥æ³•æ¡ä½œç­”ï¼‰ã€‚"}
    ]

# å±•ç¤ºå†å²æ¶ˆæ¯
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.write(m["content"])

def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "èŠå¤©è®°å½•å·²æ¸…ç©ºã€‚è¯·æå‡ºä½ çš„æ³•å¾‹é—®é¢˜ã€‚"}
    ]

st.sidebar.button("ğŸ§¹ æ¸…ç©ºèŠå¤©è®°å½•", on_click=clear_chat_history)

# å¤„ç†è¾“å…¥
prompt = st.chat_input("è¯·è¾“å…¥æ³•å¾‹ç›¸å…³é—®é¢˜...")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# ç”Ÿæˆå›ç­”
if st.session_state.messages and st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        if "query_engine" not in st.session_state or st.session_state["query_engine"] is None:
            st.error("æ£€ç´¢å¼•æ“å°šæœªå°±ç»ªï¼Œè¯·å…ˆåœ¨ä¾§è¾¹æ å®Œæˆåˆå§‹åŒ–ã€‚")
        else:
            with st.spinner("æ€è€ƒä¸­..."):
                t0 = time.time()
                try:
                    resp = st.session_state["query_engine"].query(prompt)
                    answer_text = getattr(resp, "response", str(resp))
                    placeholder = st.empty()
                    placeholder.markdown(answer_text)

                    # æº¯æºå±•ç¤º
                    if getattr(resp, "source_nodes", None):
                        st.markdown("**æ”¯æŒä¾æ®ï¼š**")
                        for i, node in enumerate(resp.source_nodes, 1):
                            meta = node.metadata or {}
                            with st.expander(f"[{i}] {meta.get('full_title','æœªçŸ¥æ ‡é¢˜')}  | ç›¸ä¼¼åº¦ï¼š{getattr(node,'score',None)}"):
                                st.write(f"æ¥æºæ–‡ä»¶ï¼š{meta.get('source_file','-')}")
                                st.write(f"æ³•å¾‹åç§°ï¼š{meta.get('law_name','-')}")
                                st.write(f"æ¡æ¬¾ï¼š{meta.get('article','-')}")
                                st.write("---")
                                st.write(node.text)

                    st.caption(f"è€—æ—¶ï¼š{time.time()-t0:.2f}s")
                except Exception as e:
                    st.error(f"æ£€ç´¢æˆ–ç”Ÿæˆå¤±è´¥ï¼š{e}")
                    answer_text = f"æ£€ç´¢æˆ–ç”Ÿæˆå¤±è´¥ï¼š{e}"

    st.session_state.messages.append({"role": "assistant", "content": answer_text})

# ------------------------ è¿è¡Œæç¤º ------------------------
with st.sidebar.expander("ğŸ“¦ ä½¿ç”¨è¯´æ˜", expanded=False):
    st.markdown(
        """
**æ•°æ®æ ¼å¼**ï¼š`./data/*.json`ï¼Œæ¯ä¸ªæ–‡ä»¶æ˜¯åˆ—è¡¨ï¼Œåˆ—è¡¨å…ƒç´ ä¸ºå­—å…¸ï¼Œå½¢å¦‚ï¼š
```json
[
  {"åŠ³åŠ¨æ³• ç¬¬ä¸‰åå…­æ¡": "â€¦â€¦æ¡æ–‡å†…å®¹â€¦â€¦"},
  {"åŠ³åŠ¨æ³• ç¬¬å››åä¸€æ¡": "â€¦â€¦æ¡æ–‡å†…å®¹â€¦â€¦"}
]
